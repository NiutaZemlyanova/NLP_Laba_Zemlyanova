{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Практическая работа «Анализ на основе RNN»**\n",
        "\n",
        "***Землянова Анна Группа 932001***\n",
        "\n",
        "*Задание:*\n",
        "\n",
        "Необходимо провести повторный анализ текста, который использовался в работе «Предобработка текста », но с использованием import rnnmorph и без использования import pymorphy2.\n",
        "\n",
        "Если полученные результаты различаются, необходимо пояснить, почему так вышло."
      ],
      "metadata": {
        "id": "Rg-Ii270K1xG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install rnnmorph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDwh62r5NHLu",
        "outputId": "8594fb00-7a3a-4241-aeba-28e35cbd08ee"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rnnmorph\n",
            "  Downloading rnnmorph-0.4.1.tar.gz (19.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.10/dist-packages (from rnnmorph) (1.23.5)\n",
            "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from rnnmorph) (1.11.3)\n",
            "Requirement already satisfied: scikit-learn>=0.18.1 in /usr/local/lib/python3.10/dist-packages (from rnnmorph) (1.2.2)\n",
            "Requirement already satisfied: keras>=2.1.4 in /usr/local/lib/python3.10/dist-packages (from rnnmorph) (2.13.1)\n",
            "Requirement already satisfied: h5py>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from rnnmorph) (3.9.0)\n",
            "Collecting pymorphy2>=0.8 (from rnnmorph)\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting russian-tagsets==0.6 (from rnnmorph)\n",
            "  Downloading russian-tagsets-0.6.tar.gz (23 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.14.0 in /usr/local/lib/python3.10/dist-packages (from rnnmorph) (4.66.1)\n",
            "Requirement already satisfied: jsonpickle>=0.9.4 in /usr/local/lib/python3.10/dist-packages (from rnnmorph) (3.0.2)\n",
            "Requirement already satisfied: nltk>=3.2.5 in /usr/local/lib/python3.10/dist-packages (from rnnmorph) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.5->rnnmorph) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.5->rnnmorph) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.5->rnnmorph) (2023.6.3)\n",
            "Collecting dawg-python>=0.7.1 (from pymorphy2>=0.8->rnnmorph)\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4 (from pymorphy2>=0.8->rnnmorph)\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt>=0.6 (from pymorphy2>=0.8->rnnmorph)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18.1->rnnmorph) (3.2.0)\n",
            "Building wheels for collected packages: rnnmorph, russian-tagsets, docopt\n",
            "  Building wheel for rnnmorph (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rnnmorph: filename=rnnmorph-0.4.1-py3-none-any.whl size=19746349 sha256=b3fed05ec276a6f84fd49b440960ff7580a3f7a99533ec244f59b8dc78663e65\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/da/6d/75c269badb3cdb41785bb5e878ce0c044ec164e4587972dc9a\n",
            "  Building wheel for russian-tagsets (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for russian-tagsets: filename=russian_tagsets-0.6-py3-none-any.whl size=24620 sha256=deb805942ac7906d20690f74992a78c386d6dee2858e93828202686e9859c04f\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/f9/53/8609e08636dbe4bad244c50e3a16ab4d824928e7342059784d\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=c3e5179be0f287bafbda92add8af94b9131db21652b5b779b9539015ae7823d6\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built rnnmorph russian-tagsets docopt\n",
            "Installing collected packages: russian-tagsets, pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2, rnnmorph\n",
            "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844 rnnmorph-0.4.1 russian-tagsets-0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "import re"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFNI3w_9A6h-",
        "outputId": "fca31ce2-7a65-499f-cc88-0faf6fab2164"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Импортируем RNNMorphPredictor и создаем экземпляр:\n",
        "from rnnmorph.predictor import RNNMorphPredictor\n",
        "predictor = RNNMorphPredictor(language='ru')"
      ],
      "metadata": {
        "id": "B6j5YWpPMG0v"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_morph_tags(tag_str):\n",
        "    # Если строка пуста, возвращаем значения \"Null\" для всех атрибутов\n",
        "    if tag_str == '_':\n",
        "        return {'Case': 'Null', 'Gender': 'Null', 'Number': 'Null'}\n",
        "\n",
        "    # Разделяем строку тегов по символу \"|\"\n",
        "    tag_parts = tag_str.split('|')\n",
        "\n",
        "    tag_dict = {}\n",
        "    # Разбираем каждую часть тега и добавляем в словарь\n",
        "    for part in tag_parts:\n",
        "        key, value = part.split('=')\n",
        "        tag_dict[key] = value\n",
        "\n",
        "    return tag_dict"
      ],
      "metadata": {
        "id": "MJpeuL07Nh46"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Прочитаем текст из файла\n",
        "with open('Text.txt', 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "\n",
        "    print(\"Текст до:\")\n",
        "    print(text)\n",
        "\n",
        "# разбиваем текст на предложения\n",
        "sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "\n",
        "#print(sentences)\n",
        "\n",
        "lem_and_tag = []\n",
        "\n",
        "for sentence in sentences:\n",
        "     words = word_tokenize(sentence)\n",
        "     #print(\"\\n\", words)\n",
        "     analyzed_words = predictor.predict(words)\n",
        "     lem_and_tag.extend([(word.pos, word.normal_form, word.tag) for word in analyzed_words])\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsXnCmtJOsqs",
        "outputId": "6352c6e9-f9f7-45c1-e83e-538d63ed1cbc"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Текст до:\n",
            "Маленький кот и большая собака пошли гулять в парк. Черный кот и белая собака бегали весело по зеленой траве. Маленькая девочка кормила кота и собаку. Счастливая девочка улыбалась.\n",
            "['Маленький кот и большая собака пошли гулять в парк.', 'Черный кот и белая собака бегали весело по зеленой траве.', 'Маленькая девочка кормила кота и собаку.', 'Счастливая девочка улыбалась.']\n",
            "\n",
            " ['Маленький', 'кот', 'и', 'большая', 'собака', 'пошли', 'гулять', 'в', 'парк', '.']\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "\n",
            " ['Черный', 'кот', 'и', 'белая', 'собака', 'бегали', 'весело', 'по', 'зеленой', 'траве', '.']\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "\n",
            " ['Маленькая', 'девочка', 'кормила', 'кота', 'и', 'собаку', '.']\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "\n",
            " ['Счастливая', 'девочка', 'улыбалась', '.']\n",
            "1/1 [==============================] - 0s 26ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Найдем пары соседних слов, удовлетворяющие условиям (совпадают по роду, числу и падежу):\n",
        "\n",
        "match_pairs = []\n",
        "\n",
        "for i in range(len(lem_and_tag) - 1):\n",
        "    pos1, word1, tag1 = lem_and_tag[i]\n",
        "    pos2, word2, tag2 = lem_and_tag[i + 1]\n",
        "\n",
        "    tag1_values = tag1.split('|')\n",
        "    tag2_values = tag2.split('|')\n",
        "\n",
        "    #print(\"tag1_values :\", tag1_values )\n",
        "    #print(\"tag2_values :\", tag2_values, \"\\n\")\n",
        "\n",
        "    tag1_dict = parse_morph_tags(tag1)\n",
        "    tag2_dict = parse_morph_tags(tag2)\n",
        "\n",
        "    if 'NOUN' in pos1 or 'NOUN' in pos2:\n",
        "        if 'ADJ' in pos1 or 'ADJ' in pos2:\n",
        "            if tag1_dict['Case']  == tag2_dict['Case']  and tag1_dict['Number'] == tag2_dict['Number'] and tag1_dict['Gender'] == tag2_dict['Gender']:\n",
        "                match_pairs.append((word1, word2))\n",
        "                #print(word1, word2)"
      ],
      "metadata": {
        "id": "sDsRfHewQivM"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Выводим найденные пары:\n",
        "for pair in match_pairs:\n",
        "    print(pair[0], pair[1])"
      ],
      "metadata": {
        "id": "KayKAh7ildSl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a92375e5-af42-40a1-9818-21fb0af3edac"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "маленький кот\n",
            "больший собака\n",
            "чёрный кот\n",
            "белый собака\n",
            "зелёный трава\n",
            "маленький девочка\n",
            "счастливый девочка\n"
          ]
        }
      ]
    }
  ]
}